{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "\n",
    "# ----\n",
    "import glob\n",
    "import pickle\n",
    "# import numpy\n",
    "from music21 import converter, instrument, note, chord\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_directory = \"../DataX/\"\n",
    "data_directory = \"DataX/\"\n",
    "charIndex_json = \"char_to_index.json\"\n",
    "model_weights_directory = 'DataX/Model_Weights/'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "        try:\n",
    "            midi = converter.parse(file)\n",
    "            notes_to_parse = None\n",
    "        except Exception as e:\n",
    "            print(\"exception at\", str(file))\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                # should make it a pitch\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    print(\"final parsing finished\")\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "#     print(\"AFTER GET NOTES\\n\", notes)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(all_chars, unique_chars):\n",
    "    length = all_chars.shape[0]\n",
    "    batch_chars = int(length / BATCH_SIZE) #155222/16 = 9701\n",
    "    \n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, 64):  #(0, 9637, 64)  #it denotes number of batches. It runs everytime when\n",
    "        #new batch is created. We have a total of 151 batches.\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    #(16, 64)\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))   #(16, 64, 87)\n",
    "        for batch_index in range(0, 16):  #it denotes each row in a batch.  \n",
    "            for i in range(0, 64):  #it denotes each column in a batch. Each column represents each character means \n",
    "                #each time-step character in a sequence.\n",
    "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
    "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1 #here we have added '1' because the\n",
    "                #correct label will be the next character in the sequence. So, the next character will be denoted by\n",
    "                #all_chars[batch_index * batch_chars + start + i + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model(batch_size, seq_length, unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(128, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(unique_chars)))\n",
    "\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    model.load_weights('DataX/Model_Weights/Weights_400.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(data, epochs = 200):\n",
    "    #mapping character to index\n",
    "    char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Number of unique characters in our whole tunes database = {}\".format(len(char_to_index))) #87\n",
    "    \n",
    "    with open(os.path.join(data_directory, charIndex_json), mode = \"w\") as f:\n",
    "        json.dump(char_to_index, f)\n",
    "        \n",
    "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
    "    unique_chars = len(char_to_index)\n",
    "    \n",
    "    model = built_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    all_characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
    "    print(\"Total number of characters = \"+str(all_characters.shape[0])) #155222\n",
    "    \n",
    "    epoch_number, loss, accuracy = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1+400, epochs))\n",
    "#         print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "\n",
    "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
    "        epoch_number.append(epoch+1)\n",
    "        \n",
    "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
    "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) #check documentation of train_on_batch here: https://keras.io/models/sequential/\n",
    "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
    "            \n",
    "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
    "        loss.append(final_epoch_loss)\n",
    "        accuracy.append(final_epoch_accuracy)\n",
    "        \n",
    "        #saving weights after every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(model_weights_directory):\n",
    "                print(\"DNE\")\n",
    "                os.makedirs(model_weights_directory)\n",
    "            model.save_weights(os.path.join(model_weights_directory, \"Weights_{}.h5\".format(epoch+1+200)))\n",
    "#             model.save_weights(model_weights_directory, \"Weights_{}.h5\".format(epoch+1))\n",
    "#           dump it to a file, instead of printing everything\n",
    "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1+400, epoch+1+400))\n",
    "#             print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1+200, epoch+1+200))\n",
    "    \n",
    "    \n",
    "    #creating dataframe and record all the losses and accuracies at each epoch\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame[\"Epoch\"] = epoch_number\n",
    "    log_frame[\"Loss\"] = loss\n",
    "    log_frame[\"Accuracy\"] = accuracy\n",
    "    log_frame.to_csv(\"DataX/log3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final parsing finished\n"
     ]
    }
   ],
   "source": [
    "data = get_notes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in our whole tunes database = 0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (16, 64, 512)             0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (16, 64, 128)             197120    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (16, 64, 128)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (16, 64, 0)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (16, 64, 0)               0         \n",
      "=================================================================\n",
      "Total params: 984,576\n",
      "Trainable params: 984,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total number of characters = 0\n",
      "Epoch 401/200\n",
      "Epoch 402/200\n",
      "Epoch 403/200\n",
      "Epoch 404/200\n",
      "Epoch 405/200\n",
      "Epoch 406/200\n",
      "Epoch 407/200\n",
      "Epoch 408/200\n",
      "Epoch 409/200\n",
      "Epoch 410/200\n",
      "Saved Weights at epoch 410 to file Weights_410.h5\n",
      "Epoch 411/200\n",
      "Epoch 412/200\n",
      "Epoch 413/200\n",
      "Epoch 414/200\n",
      "Epoch 415/200\n",
      "Epoch 416/200\n",
      "Epoch 417/200\n",
      "Epoch 418/200\n",
      "Epoch 419/200\n",
      "Epoch 420/200\n",
      "Saved Weights at epoch 420 to file Weights_420.h5\n",
      "Epoch 421/200\n",
      "Epoch 422/200\n",
      "Epoch 423/200\n",
      "Epoch 424/200\n",
      "Epoch 425/200\n",
      "Epoch 426/200\n",
      "Epoch 427/200\n",
      "Epoch 428/200\n",
      "Epoch 429/200\n",
      "Epoch 430/200\n",
      "Saved Weights at epoch 430 to file Weights_430.h5\n",
      "Epoch 431/200\n",
      "Epoch 432/200\n",
      "Epoch 433/200\n",
      "Epoch 434/200\n",
      "Epoch 435/200\n",
      "Epoch 436/200\n",
      "Epoch 437/200\n",
      "Epoch 438/200\n",
      "Epoch 439/200\n",
      "Epoch 440/200\n",
      "Saved Weights at epoch 440 to file Weights_440.h5\n",
      "Epoch 441/200\n",
      "Epoch 442/200\n",
      "Epoch 443/200\n",
      "Epoch 444/200\n",
      "Epoch 445/200\n",
      "Epoch 446/200\n",
      "Epoch 447/200\n",
      "Epoch 448/200\n",
      "Epoch 449/200\n",
      "Epoch 450/200\n",
      "Saved Weights at epoch 450 to file Weights_450.h5\n",
      "Epoch 451/200\n",
      "Epoch 452/200\n",
      "Epoch 453/200\n",
      "Epoch 454/200\n",
      "Epoch 455/200\n",
      "Epoch 456/200\n",
      "Epoch 457/200\n",
      "Epoch 458/200\n",
      "Epoch 459/200\n",
      "Epoch 460/200\n",
      "Saved Weights at epoch 460 to file Weights_460.h5\n",
      "Epoch 461/200\n",
      "Epoch 462/200\n",
      "Epoch 463/200\n",
      "Epoch 464/200\n",
      "Epoch 465/200\n",
      "Epoch 466/200\n",
      "Epoch 467/200\n",
      "Epoch 468/200\n",
      "Epoch 469/200\n",
      "Epoch 470/200\n",
      "Saved Weights at epoch 470 to file Weights_470.h5\n",
      "Epoch 471/200\n",
      "Epoch 472/200\n",
      "Epoch 473/200\n",
      "Epoch 474/200\n",
      "Epoch 475/200\n",
      "Epoch 476/200\n",
      "Epoch 477/200\n",
      "Epoch 478/200\n",
      "Epoch 479/200\n",
      "Epoch 480/200\n",
      "Saved Weights at epoch 480 to file Weights_480.h5\n",
      "Epoch 481/200\n",
      "Epoch 482/200\n",
      "Epoch 483/200\n",
      "Epoch 484/200\n",
      "Epoch 485/200\n",
      "Epoch 486/200\n",
      "Epoch 487/200\n",
      "Epoch 488/200\n",
      "Epoch 489/200\n",
      "Epoch 490/200\n",
      "Saved Weights at epoch 490 to file Weights_490.h5\n",
      "Epoch 491/200\n",
      "Epoch 492/200\n",
      "Epoch 493/200\n",
      "Epoch 494/200\n",
      "Epoch 495/200\n",
      "Epoch 496/200\n",
      "Epoch 497/200\n",
      "Epoch 498/200\n",
      "Epoch 499/200\n",
      "Epoch 500/200\n",
      "Saved Weights at epoch 500 to file Weights_500.h5\n",
      "Epoch 501/200\n",
      "Epoch 502/200\n",
      "Epoch 503/200\n",
      "Epoch 504/200\n",
      "Epoch 505/200\n",
      "Epoch 506/200\n",
      "Epoch 507/200\n",
      "Epoch 508/200\n",
      "Epoch 509/200\n",
      "Epoch 510/200\n",
      "Saved Weights at epoch 510 to file Weights_510.h5\n",
      "Epoch 511/200\n",
      "Epoch 512/200\n",
      "Epoch 513/200\n",
      "Epoch 514/200\n",
      "Epoch 515/200\n",
      "Epoch 516/200\n",
      "Epoch 517/200\n",
      "Epoch 518/200\n",
      "Epoch 519/200\n",
      "Epoch 520/200\n",
      "Saved Weights at epoch 520 to file Weights_520.h5\n",
      "Epoch 521/200\n",
      "Epoch 522/200\n",
      "Epoch 523/200\n",
      "Epoch 524/200\n",
      "Epoch 525/200\n",
      "Epoch 526/200\n",
      "Epoch 527/200\n",
      "Epoch 528/200\n",
      "Epoch 529/200\n",
      "Epoch 530/200\n",
      "Saved Weights at epoch 530 to file Weights_530.h5\n",
      "Epoch 531/200\n",
      "Epoch 532/200\n",
      "Epoch 533/200\n",
      "Epoch 534/200\n",
      "Epoch 535/200\n",
      "Epoch 536/200\n",
      "Epoch 537/200\n",
      "Epoch 538/200\n",
      "Epoch 539/200\n",
      "Epoch 540/200\n",
      "Saved Weights at epoch 540 to file Weights_540.h5\n",
      "Epoch 541/200\n",
      "Epoch 542/200\n",
      "Epoch 543/200\n",
      "Epoch 544/200\n",
      "Epoch 545/200\n",
      "Epoch 546/200\n",
      "Epoch 547/200\n",
      "Epoch 548/200\n",
      "Epoch 549/200\n",
      "Epoch 550/200\n",
      "Saved Weights at epoch 550 to file Weights_550.h5\n",
      "Epoch 551/200\n",
      "Epoch 552/200\n",
      "Epoch 553/200\n",
      "Epoch 554/200\n",
      "Epoch 555/200\n",
      "Epoch 556/200\n",
      "Epoch 557/200\n",
      "Epoch 558/200\n",
      "Epoch 559/200\n",
      "Epoch 560/200\n",
      "Saved Weights at epoch 560 to file Weights_560.h5\n",
      "Epoch 561/200\n",
      "Epoch 562/200\n",
      "Epoch 563/200\n",
      "Epoch 564/200\n",
      "Epoch 565/200\n",
      "Epoch 566/200\n",
      "Epoch 567/200\n",
      "Epoch 568/200\n",
      "Epoch 569/200\n",
      "Epoch 570/200\n",
      "Saved Weights at epoch 570 to file Weights_570.h5\n",
      "Epoch 571/200\n",
      "Epoch 572/200\n",
      "Epoch 573/200\n",
      "Epoch 574/200\n",
      "Epoch 575/200\n",
      "Epoch 576/200\n",
      "Epoch 577/200\n",
      "Epoch 578/200\n",
      "Epoch 579/200\n",
      "Epoch 580/200\n",
      "Saved Weights at epoch 580 to file Weights_580.h5\n",
      "Epoch 581/200\n",
      "Epoch 582/200\n",
      "Epoch 583/200\n",
      "Epoch 584/200\n",
      "Epoch 585/200\n",
      "Epoch 586/200\n",
      "Epoch 587/200\n",
      "Epoch 588/200\n",
      "Epoch 589/200\n",
      "Epoch 590/200\n",
      "Saved Weights at epoch 590 to file Weights_590.h5\n",
      "Epoch 591/200\n",
      "Epoch 592/200\n",
      "Epoch 593/200\n",
      "Epoch 594/200\n",
      "Epoch 595/200\n",
      "Epoch 596/200\n",
      "Epoch 597/200\n",
      "Epoch 598/200\n",
      "Epoch 599/200\n",
      "Epoch 600/200\n",
      "Saved Weights at epoch 600 to file Weights_600.h5\n"
     ]
    }
   ],
   "source": [
    "training_model(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
